# Experimental Design (Global)

Practical checklist for CS research.

1) Falsifiable question + success criteria
2) Hypotheses (H0/H1), effect size, thresholds
3) Strong baselines; keep comparisons fair
4) Control variance (seeds, paired comparisons, CIs)
5) Avoid leakage and p-hacking; pre-specify what you will report
6) Ablations to isolate what matters
7) Power/sample size where applicable; write stopping rules
8) Reproducibility: command, config, code revision
9) Reporting: mean + variability + compute budget + negatives
10) Sanity checks: smoke tests, overfit tiny batch, random-label tests

Suggested starting references:
- NeurIPS reproducibility checklist
- “Ten Simple Rules for Reproducible Computational Research” (Sandve et al.)
- “The ML Test Score” (Breck et al.)
